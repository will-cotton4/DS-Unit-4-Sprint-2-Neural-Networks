{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_434_Hyperparameter_Tuning_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ryp-TVm4njD",
        "colab_type": "text"
      },
      "source": [
        "# Your Mission, should you choose to accept it...\n",
        "\n",
        "To hyperparameter tune and extract every ounce of accuracy out of this telecom customer churn dataset: <https://drive.google.com/file/d/1dfbAsM9DwA7tYhInyflIpZnYs7VT-0AQ/view> \n",
        "\n",
        "## Requirements\n",
        "\n",
        "- Load the data\n",
        "- Clean the data if necessary (it will be)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAzYYDgfnPCl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "62c0c0e3-b0bf-41d2-9470-57690e71ac63"
      },
      "source": [
        "!pip install sklearn"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.13.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNJ-tOBs4jM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "telco_url = 'https://raw.githubusercontent.com/will-cotton4/DS-Unit-4-Sprint-2-Neural-Networks/master/module4-Hyperparameter-Tuning/telcochurn.csv'\n",
        "telco = pd.read_csv(telco_url)\n",
        "\n",
        "def clean(df):\n",
        "  df['gender'] = df['gender'].replace({'Male': 1, 'Female': 0})\n",
        "  df['Partner'] = df['Partner'].replace({'Yes': 1, 'No': 0})\n",
        "  df['Dependents'] = df['Dependents'].replace({'Yes': 1, 'No': 0})\n",
        "  df['PhoneService'] = df['PhoneService'].replace({'Yes': 1, 'No': 0})\n",
        "  df['MultipleLines'] = df['MultipleLines'].replace({'No phone service': 0, 'No': 0, 'Yes': 1})\n",
        "  \n",
        "  df['Fiber'] = df['InternetService'] == 'Fiber optic'\n",
        "  df['Fiber'] = df['Fiber'].astype(int)\n",
        "  df['DSL'] = df['InternetService'] == 'DSL'\n",
        "  df['DSL'] = df['DSL'].astype(int)\n",
        "  df['No Internet'] = df['InternetService'].where(df['InternetService'] == 'No', 0)\n",
        "  df['No Internet'] = df['No Internet'].replace({'No': 1})\n",
        "  \n",
        "  df['OnlineSecurity'] = df['OnlineSecurity'].replace({'Yes': 1, 'No': 0,'No internet service': 0})\n",
        "  df['OnlineBackup'] = df['OnlineBackup'].replace({'Yes': 1, 'No': 0,'No internet service': 0})\n",
        "  df['DeviceProtection'] = df['DeviceProtection'].replace({'Yes': 1, 'No': 0,'No internet service': 0})\n",
        "  df['TechSupport'] = df['TechSupport'].replace({'Yes': 1, 'No': 0,'No internet service': 0})\n",
        "  df['StreamingTV'] = df['StreamingTV'].replace({'Yes': 1, 'No': 0,'No internet service': 0})\n",
        "  df['StreamingMovies'] = df['StreamingMovies'].replace({'Yes': 1, 'No': 0,'No internet service': 0})\n",
        "  \n",
        "  df['Month to month'] = df['Contract'] == 'Month-to-month'\n",
        "  df['Month to month'] = df['Month to month'].astype(int)\n",
        "  df['Two year'] = df['Contract'] == 'Two year'\n",
        "  df['Two year'] = df['Two year'].astype(int)\n",
        "  df['One year'] = df['Contract'] == 'One year'\n",
        "  df['One year'] = df['One year'].astype(int)\n",
        "  \n",
        "  yearly_condition = ('One year' or 'Two year')\n",
        "  df['One or two year'] = df['Contract'] == yearly_condition\n",
        "  df['One or two year'] = df['One or two year'].astype(int)\n",
        "  \n",
        "  df['Electronic check'] = df['PaymentMethod'] == 'Electronic check'\n",
        "  df['Electronic check'] = df['Electronic check'].astype(int)\n",
        "  df['Mailed check'] = df['PaymentMethod'] == 'Mailed check'\n",
        "  df['Mailed check'] = df['Mailed check'].astype(int)\n",
        "  \n",
        "  check_condition = ('Mailed check' or 'Electronic check')\n",
        "  df['pay by check'] = df['PaymentMethod'] == check_condition\n",
        "  df['pay by check'] = df['pay by check'].astype(int)\n",
        "  \n",
        "  df['Bank transfer'] = df['PaymentMethod'] == 'Bank transfer (automatic)'\n",
        "  df['Bank transfer'] = df['Bank transfer'].astype(int)\n",
        "  df['Credit card'] = df['PaymentMethod'] == 'Credit card (automatic)'\n",
        "  df['Credit card'] = df['Credit card'].astype(int)\n",
        "  \n",
        "  auto_pay_condition = ('Bank transfer' or 'Credit card')\n",
        "  df['Auto_pay'] = df['PaymentMethod'] == auto_pay_condition\n",
        "  df['Auto_pay'] = df['Auto_pay'].astype(int)\n",
        "  \n",
        "  df['PaperlessBilling'] = df['PaperlessBilling'].replace({'Yes': 1, 'No': 0})\n",
        "  df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "  df['TotalCharges'] = df['TotalCharges'].replace({np.nan:0})\n",
        "  df['Churn'] = df['Churn'].replace({'No': 0, 'Yes':1})\n",
        "  df = df.drop(columns = ['customerID', 'Contract', 'PaymentMethod', 'InternetService'])\n",
        "  \n",
        "  X = df.drop(columns = ['Churn'])\n",
        "  y = df['Churn']\n",
        "  return X, y\n",
        "\n",
        "X, y = clean(telco)\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# y = to_categorical(y, 2)\n",
        "\n",
        "X = X.values\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucwGzXSzmtMp",
        "colab_type": "text"
      },
      "source": [
        "- Create and fit a baseline Keras MLP model to the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_0POq7oqVS4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7793510e-5a56-448a-8358-ab4685da856c"
      },
      "source": [
        "print(y_train.shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5282, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d6Sk6a4lGGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3412fd2c-a75c-4729-d57c-6eb9bb421e76"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim=29, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(25, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=150, verbose=0)\n",
        "\n",
        "scores = model.evaluate(X_train, y_train)\n",
        "print(\"train \", scores)\n",
        "scores = model.evaluate(X_test, y_test)\n",
        "print(\"test \", scores)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5282/5282 [==============================] - 0s 46us/sample - loss: 1.7877 - acc: 0.5186\n",
            "train  [1.7876619215418514, 0.51855356]\n",
            "1761/1761 [==============================] - 0s 27us/sample - loss: 1.7766 - acc: 0.5207\n",
            "test  [1.776557861709378, 0.52072686]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e4xpgfFtFZZ",
        "colab_type": "text"
      },
      "source": [
        "- Hyperparameter tune (at least) the following parameters:\n",
        " - batch_size\n",
        " - training epochs\n",
        " - optimizer\n",
        " - learning rate (if applicable to optimizer)\n",
        " - momentum (if applicable to optimizer)\n",
        " - activation functions\n",
        " - network weight initialization\n",
        " - dropout regularization\n",
        " - number of neurons in the hidden layer\n",
        " \n",
        " You must use Grid Search and Cross Validation for your initial pass of the above hyperparameters\n",
        " \n",
        " Try and get the maximum accuracy possible out of this data! You'll save big telecoms millions! Doesn't that sound great?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_VbOyXNwzWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "def create_model(optimizer='adam', activation='relu', init_mode='uniform',\n",
        "                 dropout_rate=0.2, neurons=15, lr=0.01, momentum=0):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(neurons, input_dim=29, activation=activation))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(1, activation=activation))\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGJvcjbfxGLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
        "\n",
        "param_grid = {\"batch_size\":[10],\n",
        "#               \"epochs\": [20],\n",
        "              \"optimizer\": ['adam', 'SGD'],\n",
        "              \"lr\": [0.001, 0.1],\n",
        "              \"momentum\": [0, 0.5],\n",
        "              \"activation\": ['relu', 'sigmoid'],\n",
        "              \"init_mode\":['uniform', 'glorot_normal'],\n",
        "              \"dropout_rate\": [0, 0.2, 0.6],\n",
        "              \"neurons\": [30]\n",
        "}\n",
        "\n",
        "model.check_params(param_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQz4VoLYr91j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e94558d-fdb8-4565-fd01-1d6be4d315c0"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train, epochs=100)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "5282/5282 [==============================] - 1s 151us/sample - loss: 0.5506 - acc: 0.7438\n",
            "Epoch 2/100\n",
            "5282/5282 [==============================] - 1s 139us/sample - loss: 0.5249 - acc: 0.7590\n",
            "Epoch 3/100\n",
            "5282/5282 [==============================] - 1s 129us/sample - loss: 0.5103 - acc: 0.7673\n",
            "Epoch 4/100\n",
            "5282/5282 [==============================] - 1s 128us/sample - loss: 0.5005 - acc: 0.7639\n",
            "Epoch 5/100\n",
            "5282/5282 [==============================] - 1s 127us/sample - loss: 0.5139 - acc: 0.7651\n",
            "Epoch 6/100\n",
            "5282/5282 [==============================] - 1s 128us/sample - loss: 0.5065 - acc: 0.7630\n",
            "Epoch 7/100\n",
            "5282/5282 [==============================] - 1s 127us/sample - loss: 0.5058 - acc: 0.7596\n",
            "Epoch 8/100\n",
            "5282/5282 [==============================] - 1s 126us/sample - loss: 0.4952 - acc: 0.7668\n",
            "Epoch 9/100\n",
            "5282/5282 [==============================] - 1s 128us/sample - loss: 0.5134 - acc: 0.7495\n",
            "Epoch 10/100\n",
            "5282/5282 [==============================] - 1s 126us/sample - loss: 0.4931 - acc: 0.7643\n",
            "Epoch 11/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4925 - acc: 0.7603\n",
            "Epoch 12/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4831 - acc: 0.7739\n",
            "Epoch 13/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4922 - acc: 0.7726\n",
            "Epoch 14/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4928 - acc: 0.7700\n",
            "Epoch 15/100\n",
            "5282/5282 [==============================] - 1s 127us/sample - loss: 0.4925 - acc: 0.7673\n",
            "Epoch 16/100\n",
            "5282/5282 [==============================] - 1s 126us/sample - loss: 0.4834 - acc: 0.7753\n",
            "Epoch 17/100\n",
            "5282/5282 [==============================] - 1s 126us/sample - loss: 0.4946 - acc: 0.7688\n",
            "Epoch 18/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4894 - acc: 0.7698\n",
            "Epoch 19/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4749 - acc: 0.7806\n",
            "Epoch 20/100\n",
            "5282/5282 [==============================] - 1s 123us/sample - loss: 0.4842 - acc: 0.7730\n",
            "Epoch 21/100\n",
            "5282/5282 [==============================] - 1s 123us/sample - loss: 0.4683 - acc: 0.7813\n",
            "Epoch 22/100\n",
            "5282/5282 [==============================] - 1s 123us/sample - loss: 0.4755 - acc: 0.7770\n",
            "Epoch 23/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4748 - acc: 0.7825\n",
            "Epoch 24/100\n",
            "5282/5282 [==============================] - 1s 127us/sample - loss: 0.4745 - acc: 0.7745\n",
            "Epoch 25/100\n",
            "5282/5282 [==============================] - 1s 129us/sample - loss: 0.4700 - acc: 0.7817\n",
            "Epoch 26/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4761 - acc: 0.7751\n",
            "Epoch 27/100\n",
            "5282/5282 [==============================] - 1s 126us/sample - loss: 0.4732 - acc: 0.7844\n",
            "Epoch 28/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4699 - acc: 0.7762\n",
            "Epoch 29/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4779 - acc: 0.7861\n",
            "Epoch 30/100\n",
            "5282/5282 [==============================] - 1s 126us/sample - loss: 0.4695 - acc: 0.7827\n",
            "Epoch 31/100\n",
            "5282/5282 [==============================] - 1s 127us/sample - loss: 0.4653 - acc: 0.7849\n",
            "Epoch 32/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4633 - acc: 0.7774\n",
            "Epoch 33/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4771 - acc: 0.7830\n",
            "Epoch 34/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4694 - acc: 0.7851\n",
            "Epoch 35/100\n",
            "5282/5282 [==============================] - 1s 126us/sample - loss: 0.4777 - acc: 0.7830\n",
            "Epoch 36/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4730 - acc: 0.7857\n",
            "Epoch 37/100\n",
            "5282/5282 [==============================] - 1s 123us/sample - loss: 0.4817 - acc: 0.7861\n",
            "Epoch 38/100\n",
            "5282/5282 [==============================] - 1s 123us/sample - loss: 0.4803 - acc: 0.7834\n",
            "Epoch 39/100\n",
            "5282/5282 [==============================] - 1s 126us/sample - loss: 0.4747 - acc: 0.7863\n",
            "Epoch 40/100\n",
            "5282/5282 [==============================] - 1s 127us/sample - loss: 0.4722 - acc: 0.7859\n",
            "Epoch 41/100\n",
            "5282/5282 [==============================] - 1s 129us/sample - loss: 0.4773 - acc: 0.7836\n",
            "Epoch 42/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4654 - acc: 0.7876\n",
            "Epoch 43/100\n",
            "5282/5282 [==============================] - 1s 127us/sample - loss: 0.4677 - acc: 0.7830\n",
            "Epoch 44/100\n",
            "5282/5282 [==============================] - 1s 122us/sample - loss: 0.4840 - acc: 0.7696\n",
            "Epoch 45/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4793 - acc: 0.7768\n",
            "Epoch 46/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4652 - acc: 0.7813\n",
            "Epoch 47/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4699 - acc: 0.7846\n",
            "Epoch 48/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4648 - acc: 0.7844\n",
            "Epoch 49/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4607 - acc: 0.7899\n",
            "Epoch 50/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4687 - acc: 0.7870\n",
            "Epoch 51/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4635 - acc: 0.7851\n",
            "Epoch 52/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4575 - acc: 0.7899\n",
            "Epoch 53/100\n",
            "5282/5282 [==============================] - 1s 122us/sample - loss: 0.4603 - acc: 0.7880\n",
            "Epoch 54/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4537 - acc: 0.7891\n",
            "Epoch 55/100\n",
            "5282/5282 [==============================] - 1s 128us/sample - loss: 0.4583 - acc: 0.7864\n",
            "Epoch 56/100\n",
            "5282/5282 [==============================] - 1s 123us/sample - loss: 0.4597 - acc: 0.7861\n",
            "Epoch 57/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4515 - acc: 0.7900\n",
            "Epoch 58/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4613 - acc: 0.7900\n",
            "Epoch 59/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4605 - acc: 0.7921\n",
            "Epoch 60/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4594 - acc: 0.7904\n",
            "Epoch 61/100\n",
            "5282/5282 [==============================] - 1s 123us/sample - loss: 0.4681 - acc: 0.7846\n",
            "Epoch 62/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4583 - acc: 0.7936\n",
            "Epoch 63/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4584 - acc: 0.7847\n",
            "Epoch 64/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4631 - acc: 0.7902\n",
            "Epoch 65/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4664 - acc: 0.7762\n",
            "Epoch 66/100\n",
            "5282/5282 [==============================] - 1s 123us/sample - loss: 0.4590 - acc: 0.7849\n",
            "Epoch 67/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4616 - acc: 0.7815\n",
            "Epoch 68/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4601 - acc: 0.7868\n",
            "Epoch 69/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4595 - acc: 0.7855\n",
            "Epoch 70/100\n",
            "5282/5282 [==============================] - 1s 128us/sample - loss: 0.4585 - acc: 0.7830\n",
            "Epoch 71/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4600 - acc: 0.7863\n",
            "Epoch 72/100\n",
            "5282/5282 [==============================] - 1s 126us/sample - loss: 0.4583 - acc: 0.7849\n",
            "Epoch 73/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4563 - acc: 0.7917\n",
            "Epoch 74/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4647 - acc: 0.7919\n",
            "Epoch 75/100\n",
            "5282/5282 [==============================] - 1s 127us/sample - loss: 0.4605 - acc: 0.7864\n",
            "Epoch 76/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4598 - acc: 0.7846\n",
            "Epoch 77/100\n",
            "5282/5282 [==============================] - 1s 127us/sample - loss: 0.4570 - acc: 0.7881\n",
            "Epoch 78/100\n",
            "5282/5282 [==============================] - 1s 123us/sample - loss: 0.4591 - acc: 0.7828\n",
            "Epoch 79/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4574 - acc: 0.7917\n",
            "Epoch 80/100\n",
            "5282/5282 [==============================] - 1s 128us/sample - loss: 0.4530 - acc: 0.7950\n",
            "Epoch 81/100\n",
            "5282/5282 [==============================] - 1s 128us/sample - loss: 0.4509 - acc: 0.7904\n",
            "Epoch 82/100\n",
            "5282/5282 [==============================] - 1s 126us/sample - loss: 0.4534 - acc: 0.7893\n",
            "Epoch 83/100\n",
            "5282/5282 [==============================] - 1s 126us/sample - loss: 0.4633 - acc: 0.7838\n",
            "Epoch 84/100\n",
            "5282/5282 [==============================] - 1s 127us/sample - loss: 0.4593 - acc: 0.7881\n",
            "Epoch 85/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4507 - acc: 0.7900\n",
            "Epoch 86/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4568 - acc: 0.7836\n",
            "Epoch 87/100\n",
            "5282/5282 [==============================] - 1s 128us/sample - loss: 0.4579 - acc: 0.7906\n",
            "Epoch 88/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4623 - acc: 0.7819\n",
            "Epoch 89/100\n",
            "5282/5282 [==============================] - 1s 127us/sample - loss: 0.4568 - acc: 0.7849\n",
            "Epoch 90/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4699 - acc: 0.7830\n",
            "Epoch 91/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4527 - acc: 0.7934\n",
            "Epoch 92/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4549 - acc: 0.7885\n",
            "Epoch 93/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4562 - acc: 0.7830\n",
            "Epoch 94/100\n",
            "5282/5282 [==============================] - 1s 125us/sample - loss: 0.4540 - acc: 0.7861\n",
            "Epoch 95/100\n",
            "5282/5282 [==============================] - 1s 123us/sample - loss: 0.4603 - acc: 0.7851\n",
            "Epoch 96/100\n",
            "5282/5282 [==============================] - 1s 123us/sample - loss: 0.4573 - acc: 0.7934\n",
            "Epoch 97/100\n",
            "5282/5282 [==============================] - 1s 124us/sample - loss: 0.4537 - acc: 0.7874\n",
            "Epoch 98/100\n",
            "5282/5282 [==============================] - 1s 123us/sample - loss: 0.4591 - acc: 0.7827\n",
            "Epoch 99/100\n",
            "5282/5282 [==============================] - 1s 123us/sample - loss: 0.4593 - acc: 0.7800\n",
            "Epoch 100/100\n",
            "5282/5282 [==============================] - 1s 126us/sample - loss: 0.4570 - acc: 0.7921\n",
            "Best: 0.794585 using {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.578001 (0.221465) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.578001 (0.221465) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.578001 (0.221465) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.421999 (0.221465) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.586710 (0.218202) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.569860 (0.224166) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.586710 (0.218202) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.586710 (0.218202) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.265430 (0.010375) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.578001 (0.221465) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.262401 (0.014222) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.413290 (0.218202) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.754638 (0.010474) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.743468 (0.016438) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.413290 (0.218202) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.744226 (0.006815) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.586710 (0.218202) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.754260 (0.014733) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.739114 (0.016228) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.755017 (0.014594) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.586710 (0.218202) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.745930 (0.017129) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.430140 (0.224166) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.747444 (0.021099) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.746308 (0.017473) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.413290 (0.218202) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.734570 (0.010375) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.414237 (0.217509) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.734570 (0.010375) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.759939 (0.007562) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.578001 (0.221465) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.743847 (0.016352) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.413290 (0.218202) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.734192 (0.009919) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.741197 (0.019035) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.745930 (0.017195) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'relu', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.788527 (0.006022) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.751420 (0.013624) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.761454 (0.030387) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.745172 (0.007779) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.790988 (0.007930) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.744983 (0.007577) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.787580 (0.003282) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.740818 (0.018521) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.793260 (0.007468) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.775653 (0.018383) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.744983 (0.007577) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.781711 (0.003553) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.784741 (0.012087) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.745172 (0.007779) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.781333 (0.010942) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.788148 (0.013954) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.743279 (0.016231) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.781333 (0.007196) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.751609 (0.013195) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.794585 (0.006512) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.745172 (0.007779) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.791367 (0.006803) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.788527 (0.008757) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.753502 (0.005675) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.788527 (0.009669) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.789852 (0.003653) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.753692 (0.005645) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.2, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.783983 (0.011888) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.782658 (0.006211) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.744794 (0.007379) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.758614 (0.026251) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.783226 (0.007454) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'uniform', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.787202 (0.004616) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.741764 (0.005161) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.790988 (0.006996) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.001, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.789095 (0.012849) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.735328 (0.010447) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0, 'neurons': 30, 'optimizer': 'SGD'}\n",
            "0.762969 (0.009740) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'adam'}\n",
            "0.734570 (0.010375) with: {'activation': 'sigmoid', 'batch_size': 10, 'dropout_rate': 0.6, 'init_mode': 'glorot_normal', 'lr': 0.1, 'momentum': 0.5, 'neurons': 30, 'optimizer': 'SGD'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfZRtJ7MCN3x",
        "colab_type": "text"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Try to implement Random Search Hyperparameter Tuning on this dataset\n",
        "- Try to implement Bayesian Optimiation tuning on this dataset\n",
        "- Practice hyperparameter tuning other datasets that we have looked at. How high can you get MNIST? Above 99%?\n",
        "- Study for the Sprint Challenge\n",
        " - Can you implement both perceptron and MLP models from scratch with forward and backpropagation?\n",
        " - Can you implement both perceptron and MLP models in keras and tune their hyperparameters with cross validation?"
      ]
    }
  ]
}